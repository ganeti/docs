<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>HROLLER(1) Ganeti | Version 2.9.7</title>
</head>
<body>
<div id="header">
<h1 class="title">HROLLER(1) Ganeti | Version 2.9.7</h1>
</div>
<div id="TOC">
<ul>
<li><a href="#name">NAME</a></li>
<li><a href="#synopsis">SYNOPSIS</a></li>
<li><a href="#description">DESCRIPTION</a><ul>
<li><a href="#algorithm-for-calculating-offline-reboot-groups">ALGORITHM FOR CALCULATING OFFLINE REBOOT GROUPS</a></li>
</ul></li>
<li><a href="#options">OPTIONS</a></li>
<li><a href="#bugs">BUGS</a></li>
<li><a href="#examples">EXAMPLES</a><ul>
<li><a href="#online-rolling-reboots-using-tags">Online Rolling reboots, using tags</a></li>
<li><a href="#offline-rolling-node-reboot-output">Offline Rolling node reboot output</a></li>
<li><a href="#rolling-reboots-with-non-redundant-instances">Rolling reboots with non-redundant instances</a></li>
</ul></li>
<li><a href="#reporting-bugs">REPORTING BUGS</a></li>
<li><a href="#see-also">SEE ALSO</a></li>
<li><a href="#copyright">COPYRIGHT</a></li>
</ul>
</div>
<h1 id="name"><a href="#TOC">NAME</a></h1>
<p>hroller - Cluster rolling maintenance scheduler for Ganeti</p>
<h1 id="synopsis"><a href="#TOC">SYNOPSIS</a></h1>
<p><strong>hroller</strong> {backend options...} [algorithm options...] [reporting options...]</p>
<p><strong>hroller</strong> --version</p>
<p>Backend options:</p>
<p>{ <strong>-m</strong> <em>cluster</em> | <strong>-L[</strong> <em>path</em> <strong>]</strong> | <strong>-t</strong> <em>data-file</em> | <strong>-I</strong> <em>path</em> }</p>
<p><strong>[ --force ]</strong></p>
<p>Algorithm options:</p>
<p><strong>[ -G <em>name</em> ]</strong> <strong>[ -O <em>name...</em> ]</strong> <strong>[ --node-tags</strong> <em>tag,..</em> <strong>]</strong> <strong>[ --skip-non-redundant ]</strong></p>
<p><strong>[ --offline-maintenance ]</strong> <strong>[ --ignore-non-redundant ]</strong></p>
<p>Reporting options:</p>
<p><strong>[ -v... | -q ]</strong> <strong>[ -S <em>file</em> ]</strong> <strong>[ --one-step-only ]</strong> <strong>[ --print-moves ]</strong></p>
<h1 id="description"><a href="#TOC">DESCRIPTION</a></h1>
<p>hroller is a cluster maintenance reboot scheduler. It can calculate which set of nodes can be rebooted at the same time while avoiding having both primary and secondary nodes being rebooted at the same time.</p>
<p>For backends that support identifying the master node (currently RAPI and LUXI), the master node is scheduled as the last node in the last reboot group. Apart from this restriction, larger reboot groups are put first.</p>
<h2 id="algorithm-for-calculating-offline-reboot-groups"><a href="#TOC">ALGORITHM FOR CALCULATING OFFLINE REBOOT GROUPS</a></h2>
<p>hroller will view the nodes as vertices of an undirected graph, with two kind of edges. Firstly, there are edges from the primary to the secondary node of every instance. Secondly, two nodes are connected by an edge if they are the primary nodes of two instances that have the same secondary node. It will then color the graph using a few different heuristics, and return the minimum-size color set found. Node with the same color can then simultaneously migrate all instance off to their respective secondary nodes, and it is safe to reboot them simultaneously.</p>
<h1 id="options"><a href="#TOC">OPTIONS</a></h1>
<p>For a description of the standard options check <strong>htools</strong>(1) and <strong>hbal</strong>(1).</p>
<dl>
<dt>--force</dt>
<dd><p>Do not fail, even if the master node cannot be determined.</p>
</dd>
<dt>--node-tags <em>tag,...</em></dt>
<dd><p>Restrict to nodes having at least one of the given tags.</p>
</dd>
<dt>--full-evacuation</dt>
<dd><p>Also plan moving secondaries out of the nodes to be rebooted. For each instance the move is at most a migrate (if it was primary on that node) followed by a replace secondary.</p>
</dd>
<dt>--skip-non-redundant</dt>
<dd><p>Restrict to nodes not hosting any non-redundant instance.</p>
</dd>
<dt>--offline-maintenance</dt>
<dd><p>Pretend that all instances are shutdown before the reboots are carried out. I.e., only edges from the primary to the secondary node of an instance are considered.</p>
</dd>
<dt>--ignore-non-redundnant</dt>
<dd><p>Pretend that the non-redundant instances do not exist, and only take instances with primary and secondary node into account.</p>
</dd>
<dt>--one-step-only</dt>
<dd><p>Restrict to the first reboot group. Output the group one node per line.</p>
</dd>
<dt>--print-moves</dt>
<dd><p>After each group list for each affected instance a node where it can be evacuated to. The moves are computed under the assumption that after each reboot group, all instances are moved back to their initial position.</p>
</dd>
</dl>
<h1 id="bugs"><a href="#TOC">BUGS</a></h1>
<p>If instances are online the tool should refuse to do offline rolling maintenances, unless explicitly requested.</p>
<p>End-to-end shelltests should be provided.</p>
<h1 id="examples"><a href="#TOC">EXAMPLES</a></h1>
<h2 id="online-rolling-reboots-using-tags"><a href="#TOC">Online Rolling reboots, using tags</a></h2>
<p>Selecting by tags and getting output for one step only can be used for planing the next maintenance step.</p>
<pre><code>$ hroller --node-tags needsreboot --one-step-only -L
&#39;First Reboot Group&#39;
 node1.example.com
 node3.example.com</code></pre>
<p>Typically these nodes would be drained and migrated.</p>
<pre><code>$ GROUP=`hroller --node-tags needsreboot --one-step-only --no-headers -L`
$ for node in $GROUP; do gnt-node modify -D yes $node; done
$ for node in $GROUP; do gnt-node migrate -f --submit $node; done</code></pre>
<p>After maintenance, the tags would be removed and the nodes undrained.</p>
<h2 id="offline-rolling-node-reboot-output"><a href="#TOC">Offline Rolling node reboot output</a></h2>
<p>If all instances are shut down, usually larger node groups can be found.</p>
<pre><code>$ hroller --offline-maintainance -L
&#39;Node Reboot Groups&#39;
node1.example.com,node3.example.com,node5.example.com
node8.example.com,node6.example.com,node2.example.com
node7.example.com,node4.example.com</code></pre>
<h2 id="rolling-reboots-with-non-redundant-instances"><a href="#TOC">Rolling reboots with non-redundant instances</a></h2>
<p>By default, hroller plans capacity to move the non-redundant instances out of the nodes to be rebooted. If requested, apropriate locations for the non-redundant instances can be shown. The assumption is that instances are moved back to their original node after each reboot; these back moves are not part of the output.</p>
<pre><code>$ hroller --print-moves -L
&#39;Node Reboot Groups&#39;
node-01-002,node-01-003
  inst-20 node-01-001
  inst-21 node-01-000
  inst-30 node-01-005
  inst-31 node-01-004
node-01-004,node-01-005
  inst-40 node-01-001
  inst-41 node-01-000
  inst-50 node-01-003
  inst-51 node-01-002
node-01-001,node-01-000
  inst-00 node-01-002
  inst-01 node-01-003
  inst-10 node-01-005
  inst-11 node-01-004</code></pre>
<h1 id="reporting-bugs"><a href="#TOC">REPORTING BUGS</a></h1>
<p>Report bugs to <a href="http://code.google.com/p/ganeti/">project website</a> or contact the developers using the <a href="ganeti@googlegroups.com">Ganeti mailing list</a>.</p>
<h1 id="see-also"><a href="#TOC">SEE ALSO</a></h1>
<p>Ganeti overview and specifications: <strong>ganeti</strong>(7) (general overview), <strong>ganeti-os-interface</strong>(7) (guest OS definitions), <strong>ganeti-extstorage-interface</strong>(7) (external storage providers).</p>
<p>Ganeti commands: <strong>gnt-cluster</strong>(8) (cluster-wide commands), <strong>gnt-job</strong>(8) (job-related commands), <strong>gnt-node</strong>(8) (node-related commands), <strong>gnt-instance</strong>(8) (instance commands), <strong>gnt-os</strong>(8) (guest OS commands), <strong>gnt-storage</strong>(8) (storage commands), <strong>gnt-group</strong>(8) (node group commands), <strong>gnt-backup</strong>(8) (instance import/export commands), <strong>gnt-debug</strong>(8) (debug commands).</p>
<p>Ganeti daemons: <strong>ganeti-watcher</strong>(8) (automatic instance restarter), <strong>ganeti-cleaner</strong>(8) (job queue cleaner), <strong>ganeti-noded</strong>(8) (node daemon), <strong>ganeti-masterd</strong>(8) (master daemon), <strong>ganeti-rapi</strong>(8) (remote API daemon).</p>
<p>Ganeti htools: <strong>htools</strong>(1) (generic binary), <strong>hbal</strong>(1) (cluster balancer), <strong>hspace</strong>(1) (capacity calculation), <strong>hail</strong>(1) (IAllocator plugin), <strong>hscan</strong>(1) (data gatherer from remote clusters), <strong>hinfo</strong>(1) (cluster information printer), <strong>mon-collector</strong>(7) (data collectors interface).</p>
<h1 id="copyright"><a href="#TOC">COPYRIGHT</a></h1>
<p>Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011, 2012 Google Inc. Permission is granted to copy, distribute and/or modify under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p>
<p>On Debian systems, the complete text of the GNU General Public License can be found in /usr/share/common-licenses/GPL.</p>
</body>
</html>
