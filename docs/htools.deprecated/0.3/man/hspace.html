<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>HSPACE(1) htools | Ganeti H-tools</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="date" content="" />
</head>
<body>
<h1 class="title">HSPACE(1) htools | Ganeti H-tools</h1>
<div id="name"
><h1
  >NAME</h1
  ><p
  >hspace - Cluster space analyzer for Ganeti</p
  ></div
><div id="synopsis"
><h1
  >SYNOPSIS</h1
  ><p
  ><strong
    >hspace</strong
    > {backend options...} [algorithm options...] [request options...] [ -p [<em
    >fields</em
    >] ] [-v... | -q]</p
  ><p
  ><strong
    >hspace</strong
    > --version</p
  ><p
  >Backend options:</p
  ><p
  >{ <strong
    >-m</strong
    > <em
    >cluster</em
    > | <strong
    >-L[</strong
    > <em
    >path</em
    > <strong
    >] [-X]</strong
    > | <strong
    >-t</strong
    > <em
    >data-file</em
    > | <strong
    >--simulate</strong
    > <em
    >spec</em
    > }</p
  ><p
  >Algorithm options:</p
  ><p
  ><strong
    >[ --max-cpu <em
      >cpu-ratio</em
      > ]</strong
    > <strong
    >[ --min-disk <em
      >disk-ratio</em
      > ]</strong
    > <strong
    >[ -O <em
      >name...</em
      > ]</strong
    ></p
  ><p
  >Request options:</p
  ><p
  ><strong
    >[--memory</strong
    > <em
    >mem</em
    > <strong
    >]</strong
    > <strong
    >[--disk</strong
    > <em
    >disk</em
    > <strong
    >]</strong
    > <strong
    >[--req-nodes</strong
    > <em
    >req-nodes</em
    > <strong
    >]</strong
    > <strong
    >[--vcpus</strong
    > <em
    >vcpus</em
    > <strong
    >]</strong
    > <strong
    >[--tiered-alloc</strong
    > <em
    >spec</em
    > <strong
    >]</strong
    ></p
  ></div
><div id="description"
><h1
  >DESCRIPTION</h1
  ><p
  >hspace computes how many additional instances can be fit on a cluster, while maintaining N+1 status.</p
  ><p
  >The program will try to place instances, all of the same size, on the cluster, until the point where we don't have any N+1 possible allocation. It uses the exact same allocation algorithm as the hail iallocator plugin.</p
  ><p
  >The output of the program is designed to interpreted as a shell fragment (or parsed as a <em
    >key=value</em
    > file). Options which extend the output (e.g. -p, -v) will output the additional information on stderr (such that the stdout is still parseable).</p
  ><p
  >The following keys are available in the output of the script (all prefixed with <em
    >HTS_</em
    >):</p
  ><dl
  ><dt
    >SPEC_MEM, SPEC_DSK, SPEC_CPU, SPEC_RQN</dt
    ><dd
    ><p
      >These represent the specifications of the instance model used for allocation (the memory, disk, cpu, requested nodes).</p
      ></dd
    ><dt
    >CLUSTER_MEM, CLUSTER_DSK, CLUSTER_CPU, CLUSTER_NODES</dt
    ><dd
    ><p
      >These represent the total memory, disk, CPU count and total nodes in the cluster.</p
      ></dd
    ><dt
    >INI_SCORE, FIN_SCORE</dt
    ><dd
    ><p
      >These are the initial (current) and final cluster score (see the hbal man page for details about the scoring algorithm).</p
      ></dd
    ><dt
    >INI_INST_CNT, FIN_INST_CNT</dt
    ><dd
    ><p
      >The initial and final instance count.</p
      ></dd
    ><dt
    >INI_MEM_FREE, FIN_MEM_FREE</dt
    ><dd
    ><p
      >The initial and final total free memory in the cluster (but this doesn't necessarily mean available for use).</p
      ></dd
    ><dt
    >INI_MEM_AVAIL, FIN_MEM_AVAIL</dt
    ><dd
    ><p
      >The initial and final total available memory for allocation in the cluster. If allocating redundant instances, new instances could increase the reserved memory so it doesn't necessarily mean the entirety of this memory can be used for new instance allocations.</p
      ></dd
    ><dt
    >INI_MEM_RESVD, FIN_MEM_RESVD</dt
    ><dd
    ><p
      >The initial and final reserved memory (for redundancy/N+1 purposes).</p
      ></dd
    ><dt
    >INI_MEM_INST, FIN_MEM_INST</dt
    ><dd
    ><p
      >The initial and final memory used for instances (actual runtime used RAM).</p
      ></dd
    ><dt
    >INI_MEM_OVERHEAD, FIN_MEM_OVERHEAD</dt
    ><dd
    ><p
      >The initial and final memory overhead--memory used for the node itself and unacounted memory (e.g. due to hypervisor overhead).</p
      ></dd
    ><dt
    >INI_MEM_EFF, HTS_INI_MEM_EFF</dt
    ><dd
    ><p
      >The initial and final memory efficiency, represented as instance memory divided by total memory.</p
      ></dd
    ><dt
    >INI_DSK_FREE, INI_DSK_AVAIL, INI_DSK_RESVD, INI_DSK_INST, INI_DSK_EFF</dt
    ><dd
    ><p
      >Initial disk stats, similar to the memory ones.</p
      ></dd
    ><dt
    >FIN_DSK_FREE, FIN_DSK_AVAIL, FIN_DSK_RESVD, FIN_DSK_INST, FIN_DSK_EFF</dt
    ><dd
    ><p
      >Final disk stats, similar to the memory ones.</p
      ></dd
    ><dt
    >INI_CPU_INST, FIN_CPU_INST</dt
    ><dd
    ><p
      >Initial and final number of virtual CPUs used by instances.</p
      ></dd
    ><dt
    >INI_CPU_EFF, FIN_CPU_EFF</dt
    ><dd
    ><p
      >The initial and final CPU efficiency, represented as the count of virtual instance CPUs divided by the total physical CPU count.</p
      ></dd
    ><dt
    >INI_MNODE_MEM_AVAIL, FIN_MNODE_MEM_AVAIL</dt
    ><dd
    ><p
      >The initial and final maximum per-node available memory. This is not very useful as a metric but can give an impression of the status of the nodes; as an example, this value restricts the maximum instance size that can be still created on the cluster.</p
      ></dd
    ><dt
    >INI_MNODE_DSK_AVAIL, FIN_MNODE_DSK_AVAIL</dt
    ><dd
    ><p
      >Like the above but for disk.</p
      ></dd
    ><dt
    >TSPEC</dt
    ><dd
    ><p
      >If the tiered allocation mode has been enabled, this parameter holds the pairs of specifications and counts of instances that can be created in this mode. The value of the key is a space-separated list of values; each value is of the form <em
	>memory,disk,vcpu=count</em
	> where the memory, disk and vcpu are the values for the current spec, and count is how many instances of this spec can be created. A complete value for this variable could be: <strong
	>4096,102400,2=225 2560,102400,2=20 512,102400,2=21</strong
	>.</p
      ></dd
    ><dt
    >KM_USED_CPU, KM_USED_NPU, KM_USED_MEM, KM_USED_DSK</dt
    ><dd
    ><p
      >These represents the metrics of used resources at the start of the computation (only for tiered allocation mode). The NPU value is &quot;normalized&quot; CPU count, i.e. the number of virtual CPUs divided by the maximum ratio of the virtual to physical CPUs.</p
      ></dd
    ><dt
    >KM_POOL_CPU, KM_POOL_NPU, KM_POOL_MEM, KM_POOL_DSK</dt
    ><dd
    ><p
      >These represents the total resources allocated during the tiered allocation process. In effect, they represent how much is readily available for allocation.</p
      ></dd
    ><dt
    >KM_UNAV_CPU, KM_POOL_NPU, KM_UNAV_MEM, KM_UNAV_DSK</dt
    ><dd
    ><p
      >These represents the resources left over (either free as in unallocable or allocable on their own) after the tiered allocation has been completed. They represent better the actual unallocable resources, because some other resource has been exhausted. For example, the cluster might still have 100GiB disk free, but with no memory left for instances, we cannot allocate another instance, so in effect the disk space is unallocable. Note that the CPUs here represent instance virtual CPUs, and in case the <em
	>--max-cpu</em
	> option hasn't been specified this will be -1.</p
      ></dd
    ><dt
    >ALLOC_USAGE</dt
    ><dd
    ><p
      >The current usage represented as initial number of instances divided per final number of instances.</p
      ></dd
    ><dt
    >ALLOC_COUNT</dt
    ><dd
    ><p
      >The number of instances allocated (delta between FIN_INST_CNT and INI_INST_CNT).</p
      ></dd
    ><dt
    >ALLOC_FAIL*_CNT</dt
    ><dd
    ><p
      >For the last attemp at allocations (which would have increased FIN_INST_CNT with one, if it had succeeded), this is the count of the failure reasons per failure type; currently defined are FAILMEM, FAILDISK and FAILCPU which represent errors due to not enough memory, disk and CPUs, and FAILN1 which represents a non N+1 compliant cluster on which we can't allocate instances at all.</p
      ></dd
    ><dt
    >ALLOC_FAIL_REASON</dt
    ><dd
    ><p
      >The reason for most of the failures, being one of the above FAIL* strings.</p
      ></dd
    ><dt
    >OK</dt
    ><dd
    ><p
      >A marker representing the successful end of the computation, and having value &quot;1&quot;. If this key is not present in the output it means that the computation failed and any values present should not be relied upon.</p
      ></dd
    ></dl
  ><p
  >If the tiered allocation mode is enabled, then many of the INI_/FIN_ metrics will be also displayed with a TRL_ prefix, and denote the cluster status at the end of the tiered allocation run.</p
  ></div
><div id="options"
><h1
  >OPTIONS</h1
  ><p
  >The options that can be passed to the program are as follows:</p
  ><dl
  ><dt
    >--memory <em
      >mem</em
      ></dt
    ><dd
    ><p
      >The memory size of the instances to be placed (defaults to 4GiB).</p
      ></dd
    ><dt
    >--disk <em
      >disk</em
      ></dt
    ><dd
    ><p
      >The disk size of the instances to be placed (defaults to 100GiB).</p
      ></dd
    ><dt
    >--req-nodes <em
      >num-nodes</em
      ></dt
    ><dd
    ><p
      >The number of nodes for the instances; the default of two means mirrored instances, while passing one means plain type instances.</p
      ></dd
    ><dt
    >--vcpus <em
      >vcpus</em
      ></dt
    ><dd
    ><p
      >The number of VCPUs of the instances to be placed (defaults to 1).</p
      ></dd
    ><dt
    >--max-cpu=<em
      >cpu-ratio</em
      ></dt
    ><dd
    ><p
      >The maximum virtual to physical cpu ratio, as a floating point number between zero and one. For example, specifying <em
	>cpu-ratio</em
	> as <strong
	>2.5</strong
	> means that, for a 4-cpu machine, a maximum of 10 virtual cpus should be allowed to be in use for primary instances. A value of one doesn't make sense though, as that means no disk space can be used on it.</p
      ></dd
    ><dt
    >--min-disk=<em
      >disk-ratio</em
      ></dt
    ><dd
    ><p
      >The minimum amount of free disk space remaining, as a floating point number. For example, specifying <em
	>disk-ratio</em
	> as <strong
	>0.25</strong
	> means that at least one quarter of disk space should be left free on nodes.</p
      ></dd
    ><dt
    >-p, --print-nodes</dt
    ><dd
    ><p
      >Prints the before and after node status, in a format designed to allow the user to understand the node's most important parameters.</p
      ><p
      >It is possible to customise the listed information by passing a comma-separated list of field names to this option (the field list is currently undocumented), or to extend the default field list by prefixing the additional field list with a plus sign. By default, the node list will contain the following information:</p
      ><dl
      ><dt
	>F</dt
	><dd
	><p
	  >a character denoting the status of the node, with '-' meaning an offline node, '*' meaning N+1 failure and blank meaning a good node</p
	  ></dd
	><dt
	>Name</dt
	><dd
	><p
	  >the node name</p
	  ></dd
	><dt
	>t_mem</dt
	><dd
	><p
	  >the total node memory</p
	  ></dd
	><dt
	>n_mem</dt
	><dd
	><p
	  >the memory used by the node itself</p
	  ></dd
	><dt
	>i_mem</dt
	><dd
	><p
	  >the memory used by instances</p
	  ></dd
	><dt
	>x_mem</dt
	><dd
	><p
	  >amount memory which seems to be in use but cannot be determined why or by which instance; usually this means that the hypervisor has some overhead or that there are other reporting errors</p
	  ></dd
	><dt
	>f_mem</dt
	><dd
	><p
	  >the free node memory</p
	  ></dd
	><dt
	>r_mem</dt
	><dd
	><p
	  >the reserved node memory, which is the amount of free memory needed for N+1 compliance</p
	  ></dd
	><dt
	>t_dsk</dt
	><dd
	><p
	  >total disk</p
	  ></dd
	><dt
	>f_dsk</dt
	><dd
	><p
	  >free disk</p
	  ></dd
	><dt
	>pcpu</dt
	><dd
	><p
	  >the number of physical cpus on the node</p
	  ></dd
	><dt
	>vcpu</dt
	><dd
	><p
	  >the number of virtual cpus allocated to primary instances</p
	  ></dd
	><dt
	>pcnt</dt
	><dd
	><p
	  >number of primary instances</p
	  ></dd
	><dt
	>scnt</dt
	><dd
	><p
	  >number of secondary instances</p
	  ></dd
	><dt
	>p_fmem</dt
	><dd
	><p
	  >percent of free memory</p
	  ></dd
	><dt
	>p_fdsk</dt
	><dd
	><p
	  >percent of free disk</p
	  ></dd
	><dt
	>r_cpu</dt
	><dd
	><p
	  >ratio of virtual to physical cpus</p
	  ></dd
	><dt
	>lCpu</dt
	><dd
	><p
	  >the dynamic CPU load (if the information is available)</p
	  ></dd
	><dt
	>lMem</dt
	><dd
	><p
	  >the dynamic memory load (if the information is available)</p
	  ></dd
	><dt
	>lDsk</dt
	><dd
	><p
	  >the dynamic disk load (if the information is available)</p
	  ></dd
	><dt
	>lNet</dt
	><dd
	><p
	  >the dynamic net load (if the information is available)</p
	  ></dd
	></dl
      ></dd
    ><dt
    >-O <em
      >name</em
      ></dt
    ><dd
    ><p
      >This option (which can be given multiple times) will mark nodes as being <em
	>offline</em
	>. This means a couple of things:</p
      ><ul
      ><li
	>instances won't be placed on these nodes, not even temporarily;e.g. the <em
	  >replace primary</em
	  > move is not available if the secondary node is offline, since this move requires a failover.</li
	><li
	>these nodes will not be included in the score calculation (except for the percentage of instances on offline nodes)</li
	></ul
      ><p
      >Note that the algorithm will also mark as offline any nodes which are reported by RAPI as such, or that have &quot;?&quot; in file-based input in any numeric fields.</p
      ></dd
    ><dt
    >-t <em
      >datafile</em
      >, --text-data=<em
      >datafile</em
      ></dt
    ><dd
    ><p
      >The name of the file holding node and instance information (if not collecting via RAPI or LUXI). This or one of the other backends must be selected.</p
      ></dd
    ><dt
    >-S <em
      >filename</em
      >, --save-cluster=<em
      >filename</em
      ></dt
    ><dd
    ><p
      >If given, the state of the cluster at the end of the allocation is saved to a file named <em
	>filename.alloc</em
	>, and if tiered allocation is enabled, the state after tiered allocation will be saved to <em
	>filename.tiered</em
	>. This allows re-feeding the cluster state to either hspace itself (with different parameters) or for example hbal.</p
      ></dd
    ><dt
    >-m <em
      >cluster</em
      ></dt
    ><dd
    ><p
      >Collect data directly from the <em
	>cluster</em
	> given as an argument via RAPI. If the argument doesn't contain a colon (:), then it is converted into a fully-built URL via prepending <code
	>https://</code
	> and appending the default RAPI port, otherwise it's considered a fully-specified URL and is used as-is.</p
      ></dd
    ><dt
    >-L [<em
      >path</em
      >]</dt
    ><dd
    ><p
      >Collect data directly from the master daemon, which is to be contacted via the luxi (an internal Ganeti protocol). An optional <em
	>path</em
	> argument is interpreted as the path to the unix socket on which the master daemon listens; otherwise, the default path used by ganeti when installed with <em
	>--localstatedir=/var</em
	> is used.</p
      ></dd
    ><dt
    >--simulate <em
      >description</em
      ></dt
    ><dd
    ><p
      >Instead of using actual data, build an empty cluster given a node description. The <em
	>description</em
	> parameter must be a comma-separated list of five elements, describing in order:</p
      ><ul
      ><li
	>the allocation policy for this node group</li
	><li
	>the number of nodes in the cluster</li
	><li
	>the disk size of the nodes, in mebibytes</li
	><li
	>the memory size of the nodes, in mebibytes</li
	><li
	>the cpu core count for the nodes</li
	></ul
      ><p
      >An example description would be <strong
	>preferred,B20,102400,16384,4</strong
	> describing a 20-node cluster where each node has 100GiB of disk space, 16GiB of memory and 4 CPU cores. Note that all nodes must have the same specs currently.</p
      ><p
      >This option can be given multiple times, and each new use defines a new node group. Hence different node groups can have different allocation policies and node count/specifications.</p
      ></dd
    ><dt
    >--tiered-alloc <em
      >spec</em
      ></dt
    ><dd
    ><p
      >Besides the standard, fixed-size allocation, also do a tiered allocation scheme where the algorithm starts from the given specification and allocates until there is no more space; then it decreases the specification and tries the allocation again. The decrease is done on the matric that last failed during allocation. The specification given is similar to the <em
	>--simulate</em
	> option and it holds:</p
      ><ul
      ><li
	>the disk size of the instance</li
	><li
	>the memory size of the instance</li
	><li
	>the vcpu count for the insance</li
	></ul
      ><p
      >An example description would be <em
	>10240,8192,2</em
	> describing an initial starting specification of 10GiB of disk space, 4GiB of memory and 2 VCPUs.</p
      ><p
      >Also note that the normal allocation and the tiered allocation are independent, and both start from the initial cluster state; as such, the instance count for these two modes are not related one to another.</p
      ></dd
    ><dt
    >-v, --verbose</dt
    ><dd
    ><p
      >Increase the output verbosity. Each usage of this option will increase the verbosity (currently more than 2 doesn't make sense) from the default of one.</p
      ></dd
    ><dt
    >-q, --quiet</dt
    ><dd
    ><p
      >Decrease the output verbosity. Each usage of this option will decrease the verbosity (less than zero doesn't make sense) from the default of one.</p
      ></dd
    ><dt
    >-V, --version</dt
    ><dd
    ><p
      >Just show the program version and exit.</p
      ></dd
    ></dl
  ></div
><div id="exit-status"
><h1
  >EXIT STATUS</h1
  ><p
  >The exist status of the command will be zero, unless for some reason the algorithm fatally failed (e.g. wrong node or instance data).</p
  ></div
><div id="bugs"
><h1
  >BUGS</h1
  ><p
  >The algorithm is highly dependent on the number of nodes; its runtime grows exponentially with this number, and as such is impractical for really big clusters.</p
  ><p
  >The algorithm doesn't rebalance the cluster or try to get the optimal fit; it just allocates in the best place for the current step, without taking into consideration the impact on future placements.</p
  ></div
><div id="see-also"
><h1
  >SEE ALSO</h1
  ><p
  ><strong
    >hbal</strong
    >(1), <strong
    >hscan</strong
    >(1), <strong
    >hail</strong
    >(1), <strong
    >ganeti</strong
    >(7), <strong
    >gnt-instance</strong
    >(8), <strong
    >gnt-node</strong
    >(8)</p
  ></div
><div id="copyright"
><h1
  >COPYRIGHT</h1
  ><p
  >Copyright (C) 2009, 2010, 2011 Google Inc. Permission is granted to copy, distribute and/or modify under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p
  ><p
  >On Debian systems, the complete text of the GNU General Public License can be found in /usr/share/common-licenses/GPL.</p
  ></div
>
</body>
</html>

